“Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. It is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmann–Landau notation or asymptotic notation.”
— Wikipedia’s definition of Big O notation
In plain words, Big O notation describes the complexity of your code using algebraic terms.

To understand what Big O notation is, we can take a look at a typical example, O(n²), which is usually pronounced “Big O squared”. The letter “n” here represents the input size, and the function “g(n) = n²” inside the “O()” gives us an idea of how complex the algorithm is with respect to the input size.

A typical algorithm that has the complexity of O(n²) would be the selection sort algorithm. Selection sort is a sorting algorithm that iterates through the list to ensure every element at index i is the ith smallest/largest element of the list. The CODEPEN below gives a visual example of it.

SelectionSort(List) {
  for(i from 0 to List.Length) {
    SmallestElement = List[i]
    for(j from i to List.Length) {
      if(SmallestElement > List[j]) {
        SmallestElement = List[j]
      }
    }
    Swap(List[i], SmallestElement)
  }
}


In this scenario, we consider the variable List as the input, thus input size n is the number of elements inside List. Assume the if statement, and the value assignment bounded by the if statement, takes constant time. Then we can find the big O notation for the SelectionSort function by analyzing how many times the statements are executed.

First the inner for loop runs the statements inside n times. And then after i is incremented, the inner for loop runs for n-1 times… …until it runs once, then both of the for loops reach their terminating conditions.


big-O notation
(definition)

Definition: A theoretical measure of the execution of an algorithm, usually the time or memory needed, given the problem size n, which is usually the number of items. Informally, saying some equation f(n) = O(g(n)) means it is less than some constant multiple of g(n). The notation is read, "f of n is big oh of g of n".

Formal Definition: f(n) = O(g(n)) means there are positive constants c and k, such that 0 ≤ f(n) ≤ cg(n) for all n ≥ k. The values of c and k must be fixed for the function f and must not depend on n.
graph showing relation between a function, f, and the limit function, g

Big O runtimes are heavily generalized. We typically default to using the variable n as the input that our algorithm receives. Although we will never know exactly what the input’s size is, we must assume that it is extremely large.
We take the most expensive set of computations around a block of code and revolve the efficiency of the algorithm based around that.
At the end of this article, we will be able to make an expression that includes the runtime for the majority of computations within a certain block of code.
Here’s an example expression of what we might see:
O(n) + O(n^2) + O(1)
In this expression, we take the runtime with the heaviest cost and call that the Big O runtime of the algorithm. In this imaginary example, the runtime is O(n²) as that computation is what will take the most time in the given expression.
But wait…what is n in this example? When specifying a runtime, it is just as important to clarify what n refers to. In this example, we can say something along the lines of: “The runtime would be O(n²) where n is the length of our input array”.
In layman’s terms, we can say that that Big O is a way to measure how something performs given an N input.
Another rule of thumb when dealing with Big O: we drop constants in our expression. For example, O(n) + O(n) + O(n) will turn into O(3n), but we never actually claim our runtime to be O(3n). Instead, we simply call it to be O(n).
Now that we have a basic understanding of different complexities, let’s examine a few of the common runtimes to see how they might look in code.
